{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Custom LLM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### MULTI HEAD ATTENTION ###############################\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### LAYER NORMALIZATION & FEED FORWARD NETWORK ###############################\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### TRANSFORMER BLOCK ###############################\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### GPT MODEL ###############################\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Generate Text ###############################\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Model training ###############################\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.662, Val loss nan\n",
      "Every effort moves you, the, the, the, the, the., the, the,.,, the, the, the, the, the,,,,,,,, the, the.,,,, the,,., the\n",
      "Every effort moves you, the, the, the, the, the, the, the, the. , the, the, the, the, the, the, the, the, the, the,, the, the, the, the. ,\n",
      "Ep 3 (Step 000005): Train loss 7.212, Val loss nan\n",
      "Every effort moves you the the the the. \", the the. \", the. \", the, the the. \", the the the, the, the. \", the. \", the. \". \"\n",
      "Every effort moves you the the \", and, the the the the the \", and, and \", and, and, and, and the the, and, and \", and, and, and, and, and, and, and\n",
      "Every effort moves you the the the the the the the the the the the the. \". \"-- the. \". \" a, and the. \" a. \" a, and the. \" a had the. \"\n",
      "Ep 6 (Step 000010): Train loss 5.279, Val loss nan\n",
      "Every effort moves you--I, and in the to the the of the--I, and, and I had been, and in the, and I had been the, and I had been of the, and, and, and, and I had been, and\n",
      "Every effort moves you the the I had been the, I had been the \". \"--I, and I had been of the, I had been the house, I had been of his pictures, I had been of the, I had been. Gis\n",
      "Ep 8 (Step 000015): Train loss 3.688, Val loss nan\n",
      "Every effort moves you know, in the. Gisburn's the. Gisburn's the. I, in a little the. I to me to, in the, and in the, and, and, and, and in a little. \n",
      "Every effort moves you know and in the. \" the the. \"--and a little the picture--and it was his pictures. \"Oh, I had been the, it--and, so. \"Oh, with a little the. \n",
      "Every effort moves you know, in the. \"Oh, as a little--the a little--the was, it was not to have to me to see it, in the, it was,, as. \"Oh, as a little to have been\n",
      "Training completed in 1.38 minutes.\n"
     ]
    }
   ],
   "source": [
    "################################ Initiate model training ###############################\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "\n",
    "# gpt = GPTModel(NEW_CONFIG)\n",
    "# gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before loading the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Emb Explosion Forwardpharocax Extra condem JoshÃ´ Introdu debut088anityAD risesNot dignityalmostisel halt benign Vietnameseitals 701\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrav/miniconda3/envs/llmenv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download3 import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################### MULTI HEAD ATTENTION ###############################\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# import tiktoken\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "#         super().__init__()\n",
    "#         assert (d_out % num_heads == 0), \\\n",
    "#             \"d_out must be divisible by num_heads\"\n",
    "\n",
    "#         self.d_out = d_out\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "#         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.register_buffer(\n",
    "#             \"mask\",\n",
    "#             torch.triu(torch.ones(context_length, context_length),\n",
    "#                        diagonal=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         b, num_tokens, d_in = x.shape\n",
    "\n",
    "#         keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "#         queries = self.W_query(x)\n",
    "#         values = self.W_value(x)\n",
    "\n",
    "#         # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "#         # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "#         keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "#         values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "#         queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "#         # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "#         keys = keys.transpose(1, 2)\n",
    "#         queries = queries.transpose(1, 2)\n",
    "#         values = values.transpose(1, 2)\n",
    "\n",
    "#         # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "#         attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "#         # Original mask truncated to the number of tokens and converted to boolean\n",
    "#         mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "#         # Use the mask to fill attention scores\n",
    "#         attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "#         attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "#         attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "#         # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "#         context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "#         # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "#         context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "#         context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "#         return context_vec\n",
    "    \n",
    "\n",
    "#     ############################### LAYER NORMALIZATION & FEED FORWARD NETWORK ###############################\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "#             GELU(), ## Activation\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "    \n",
    "\n",
    "\n",
    "#     ############################### TRANSFORMER BLOCK ###############################\n",
    "\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             context_length=cfg[\"context_length\"],\n",
    "#             num_heads=cfg[\"n_heads\"], \n",
    "#             dropout=cfg[\"drop_rate\"],\n",
    "#             qkv_bias=cfg[\"qkv_bias\"])\n",
    "#         self.ff = FeedForward(cfg)\n",
    "#         self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Shortcut connection for attention block\n",
    "#         shortcut = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         # Shortcut connection for feed forward block\n",
    "#         shortcut = x\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.ff(x)\n",
    "#         # 2*4*768\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         return x\n",
    "#         # 2*4*768\n",
    "\n",
    "\n",
    "# ############################### GPT MODEL ###############################\n",
    "\n",
    "# class GPTModel(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "#         self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "#         self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "#         self.trf_blocks = nn.Sequential(\n",
    "#             *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "#         self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.out_head = nn.Linear(\n",
    "#             cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "#         )\n",
    "\n",
    "#     def forward(self, in_idx):\n",
    "#         batch_size, seq_len = in_idx.shape\n",
    "#         tok_embeds = self.tok_emb(in_idx)\n",
    "#         pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "#         x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_emb(x)\n",
    "#         x = self.trf_blocks(x)\n",
    "#         x = self.final_norm(x)\n",
    "#         logits = self.out_head(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# ############################### Generate Text ###############################\n",
    "\n",
    "# def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "#     # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "#     for _ in range(max_new_tokens):\n",
    "#         idx_cond = idx[:, -context_size:]\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(idx_cond)\n",
    "#         logits = logits[:, -1, :]\n",
    "\n",
    "#         # New: Filter logits with top_k sampling\n",
    "#         if top_k is not None:\n",
    "#             # Keep only top_k values\n",
    "#             top_logits, _ = torch.topk(logits, top_k)\n",
    "#             min_val = top_logits[:, -1]\n",
    "#             logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "#         # New: Apply temperature scaling\n",
    "#         if temperature > 0.0:\n",
    "#             logits = logits / temperature\n",
    "\n",
    "#             # Apply softmax to get probabilities\n",
    "#             probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "#             # Sample from the distribution\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "#         # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "#         else:\n",
    "#             idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "#         if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "#             break\n",
    "\n",
    "#         # Same as before: append sampled index to the running sequence\n",
    "#         idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "#     return idx\n",
    "\n",
    "\n",
    "# # Define model configurations in a dictionary for compactness\n",
    "# model_configs = {\n",
    "#     \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "#     \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "#     \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "#     \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "# }\n",
    "\n",
    "# GPT_CONFIG_124M = {\n",
    "#     \"vocab_size\": 50257,    # Vocabulary size\n",
    "#     \"context_length\": 1024, # Context length\n",
    "#     \"emb_dim\": 768,         # Embedding dimension\n",
    "#     \"n_heads\": 12,          # Number of attention heads\n",
    "#     \"n_layers\": 12,         # Number of layers\n",
    "#     \"drop_rate\": 0.1,       # Dropout rate\n",
    "#     \"qkv_bias\": False       # Query-Key-Value bias\n",
    "# }\n",
    "\n",
    "# # Copy the base configuration and update with specific model settings\n",
    "# model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "# NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "# NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "# NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "# gpt = GPTModel(NEW_CONFIG)\n",
    "# gpt.eval();\n",
    "\n",
    "# # gpt = GPTModel(NEW_CONFIG)\n",
    "# # gpt.eval();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define model configurations in a dictionary for compactness\n",
    "# model_configs = {\n",
    "#     \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "#     \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "#     \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "#     \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "# }\n",
    "\n",
    "# GPT_CONFIG_124M = {\n",
    "#     \"vocab_size\": 50257,    # Vocabulary size\n",
    "#     \"context_length\": 1024, # Context length\n",
    "#     \"emb_dim\": 768,         # Embedding dimension\n",
    "#     \"n_heads\": 12,          # Number of attention heads\n",
    "#     \"n_layers\": 12,         # Number of layers\n",
    "#     \"drop_rate\": 0.1,       # Dropout rate\n",
    "#     \"qkv_bias\": False       # Query-Key-Value bias\n",
    "# }\n",
    "\n",
    "# # Copy the base configuration and update with specific model settings\n",
    "# model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "# NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "# NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "# NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "# gpt = GPTModel(NEW_CONFIG)\n",
    "# gpt.eval();\n",
    "\n",
    "# # gpt = GPTModel(NEW_CONFIG)\n",
    "# # gpt.eval();\n",
    "\n",
    "# def text_to_token_ids(text, tokenizer):\n",
    "#     encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "#     encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "#     return encoded_tensor\n",
    "\n",
    "# def token_ids_to_text(token_ids, tokenizer):\n",
    "#     flat = token_ids.squeeze(0) # remove batch dimension\n",
    "#     return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# token_ids = generate(\n",
    "#     model=gpt,\n",
    "#     idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "#     max_new_tokens=25,\n",
    "#     context_size=NEW_CONFIG[\"context_length\"],\n",
    "#     top_k=50,\n",
    "#     temperature=1.5\n",
    "# )\n",
    "\n",
    "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "# from gpt_download3 import download_and_load_gpt2\n",
    "# settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def assign(left, right):\n",
    "#     if left.shape != right.shape:\n",
    "#         raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "#     return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "# def load_weights_into_gpt(gpt, params):\n",
    "#     gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "#     gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "#     for b in range(len(params[\"blocks\"])):\n",
    "#         q_w, k_w, v_w = np.split(\n",
    "#             (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "#         gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "#         gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "#         gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "#         q_b, k_b, v_b = np.split(\n",
    "#             (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "#         gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "#         gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "#         gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "#         gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.out_proj.weight, \n",
    "#             params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "#         gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.out_proj.bias, \n",
    "#             params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "#         gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "#             gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "#             params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "#         gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "#             gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "#             params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "#         gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "#             gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "#             params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "#         gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "#             gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "#             params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "#         gpt.trf_blocks[b].norm1.scale = assign(\n",
    "#             gpt.trf_blocks[b].norm1.scale, \n",
    "#             params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "#         gpt.trf_blocks[b].norm1.shift = assign(\n",
    "#             gpt.trf_blocks[b].norm1.shift, \n",
    "#             params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "#         gpt.trf_blocks[b].norm2.scale = assign(\n",
    "#             gpt.trf_blocks[b].norm2.scale, \n",
    "#             params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "#         gpt.trf_blocks[b].norm2.shift = assign(\n",
    "#             gpt.trf_blocks[b].norm2.shift, \n",
    "#             params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "#     gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "#     gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "#     gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "# load_weights_into_gpt(gpt, params)\n",
    "# gpt.to(device);\n",
    "\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "\n",
    "# token_ids = generate(\n",
    "#     model=gpt,\n",
    "#     idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "#     max_new_tokens=25,\n",
    "#     context_size=NEW_CONFIG[\"context_length\"],\n",
    "#     top_k=50,\n",
    "#     temperature=1.5\n",
    "# )\n",
    "\n",
    "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
